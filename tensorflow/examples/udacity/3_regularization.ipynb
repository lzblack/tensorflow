{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "![](https://raw.githubusercontent.com/ritchieng/machine-learning-nanodegree/master/deep_learning/deep_neural_nets/dnn12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic model\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "        + beta * tf.nn.l2_loss(weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.050961\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 10.6%\n",
      "Minibatch loss at step 500: 3.529849\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 1000: 1.681666\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1500: 1.350154\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 2000: 1.002482\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2500: 0.979264\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 3000: 0.755158\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 88.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, beta: 0.001}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.0001\n",
      "Initialized\n",
      "Accuracy: 86.97 \n",
      "\n",
      "beta: 0.000125892541179\n",
      "Initialized\n",
      "Accuracy: 86.77 \n",
      "\n",
      "beta: 0.000158489319246\n",
      "Initialized\n",
      "Accuracy: 86.68 \n",
      "\n",
      "beta: 0.000199526231497\n",
      "Initialized\n",
      "Accuracy: 87.31 \n",
      "\n",
      "beta: 0.000251188643151\n",
      "Initialized\n",
      "Accuracy: 87.5 \n",
      "\n",
      "beta: 0.000316227766017\n",
      "Initialized\n",
      "Accuracy: 88.03 \n",
      "\n",
      "beta: 0.000398107170553\n",
      "Initialized\n",
      "Accuracy: 87.98 \n",
      "\n",
      "beta: 0.000501187233627\n",
      "Initialized\n",
      "Accuracy: 88.02 \n",
      "\n",
      "beta: 0.00063095734448\n",
      "Initialized\n",
      "Accuracy: 88.56 \n",
      "\n",
      "beta: 0.000794328234724\n",
      "Initialized\n",
      "Accuracy: 88.93 \n",
      "\n",
      "beta: 0.001\n",
      "Initialized\n",
      "Accuracy: 89.0 \n",
      "\n",
      "beta: 0.00125892541179\n",
      "Initialized\n",
      "Accuracy: 88.94 \n",
      "\n",
      "beta: 0.00158489319246\n",
      "Initialized\n",
      "Accuracy: 89.11 \n",
      "\n",
      "beta: 0.00199526231497\n",
      "Initialized\n",
      "Accuracy: 88.96 \n",
      "\n",
      "beta: 0.00251188643151\n",
      "Initialized\n",
      "Accuracy: 88.94 \n",
      "\n",
      "beta: 0.00316227766017\n",
      "Initialized\n",
      "Accuracy: 88.9 \n",
      "\n",
      "beta: 0.00398107170553\n",
      "Initialized\n",
      "Accuracy: 88.94 \n",
      "\n",
      "beta: 0.00501187233627\n",
      "Initialized\n",
      "Accuracy: 88.99 \n",
      "\n",
      "beta: 0.0063095734448\n",
      "Initialized\n",
      "Accuracy: 89.06 \n",
      "\n",
      "beta: 0.00794328234724\n",
      "Initialized\n",
      "Accuracy: 89.08 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# again, tuning the new parameter\n",
    "\n",
    "num_steps = 3001\n",
    "beta_vals = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for beta_val in beta_vals:\n",
    "    print(\"beta:\", beta_val)\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, beta: beta_val}\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    "        print('Accuracy:', accuracy_val[-1], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpJAFPoiIglEoS8ikkD+P8kIOxAwzh98AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f790071e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.semilogx(beta_vals, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN with relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN with relu\n",
    "\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "        + beta*(tf.nn.l2_loss(weights1)+tf.nn.l2_loss(weights2)) )\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3493.973145\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 30.8%\n",
      "Minibatch loss at step 500: 21.517811\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1000: 0.886992\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1500: 0.803465\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 2000: 0.712140\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 2500: 0.802180\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 3000: 0.731147\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.2%\n",
      "Test accuracy: 90.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, beta:0.01}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 0.0001\n",
      "Initialized\n",
      "Accuracy: 89.62 \n",
      "\n",
      "beta: 0.000125892541179\n",
      "Initialized\n",
      "Accuracy: 89.46 \n",
      "\n",
      "beta: 0.000158489319246\n",
      "Initialized\n",
      "Accuracy: 89.57 \n",
      "\n",
      "beta: 0.000199526231497\n",
      "Initialized\n",
      "Accuracy: 89.12 \n",
      "\n",
      "beta: 0.000251188643151\n",
      "Initialized\n",
      "Accuracy: 89.15 \n",
      "\n",
      "beta: 0.000316227766017\n",
      "Initialized\n",
      "Accuracy: 90.5 \n",
      "\n",
      "beta: 0.000398107170553\n",
      "Initialized\n",
      "Accuracy: 91.14 \n",
      "\n",
      "beta: 0.000501187233627\n",
      "Initialized\n",
      "Accuracy: 91.31 \n",
      "\n",
      "beta: 0.00063095734448\n",
      "Initialized\n",
      "Accuracy: 92.01 \n",
      "\n",
      "beta: 0.000794328234724\n",
      "Initialized\n",
      "Accuracy: 92.53 \n",
      "\n",
      "beta: 0.001\n",
      "Initialized\n",
      "Accuracy: 93.28 \n",
      "\n",
      "beta: 0.00125892541179\n",
      "Initialized\n",
      "Accuracy: 93.52 \n",
      "\n",
      "beta: 0.00158489319246\n",
      "Initialized\n",
      "Accuracy: 93.75 \n",
      "\n",
      "beta: 0.00199526231497\n",
      "Initialized\n",
      "Accuracy: 93.38 \n",
      "\n",
      "beta: 0.00251188643151\n",
      "Initialized\n",
      "Accuracy: 92.91 \n",
      "\n",
      "beta: 0.00316227766017\n",
      "Initialized\n",
      "Accuracy: 92.85 \n",
      "\n",
      "beta: 0.00398107170553\n",
      "Initialized\n",
      "Accuracy: 92.31 \n",
      "\n",
      "beta: 0.00501187233627\n",
      "Initialized\n",
      "Accuracy: 91.92 \n",
      "\n",
      "beta: 0.0063095734448\n",
      "Initialized\n",
      "Accuracy: 91.46 \n",
      "\n",
      "beta: 0.00794328234724\n",
      "Initialized\n",
      "Accuracy: 90.93 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# again, tuning the new parameter\n",
    "\n",
    "num_steps = 3001\n",
    "beta_vals = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for beta_val in beta_vals:\n",
    "    print(\"beta:\", beta_val)\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, beta: beta_val}\n",
    "            _, l, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    "        print('Accuracy:', accuracy_val[-1], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEMCAYAAADUEk3/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYpoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f97685e908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.semilogx(beta_vals, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN with relu\n",
    "\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)\n",
    "        + beta*(tf.nn.l2_loss(weights1)+tf.nn.l2_loss(weights2)) )\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step % num_batches) * batch_size % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, beta:0.01}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 2 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
